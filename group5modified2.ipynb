{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab87fa1a",
   "metadata": {},
   "source": [
    "Everything is the same: \n",
    "Only differences are the following. \n",
    "A pre train split for the short term and the long term LSTM embeddinsg to get more accurate results\n",
    "In the final model, there is an 80-20 split for the 309 days taken (that is not taken for the pre training). This 20% is the old test set (TEST1)\n",
    "Then there is a new test split for the dates given in the pdf. That is TEST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fab9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the Nifty500 list\n",
    "nifty500_list = pd.read_csv('ind_nifty500list_filtered_final.csv')\n",
    "nifty500_tickers = nifty500_list['Symbol'] + '.NS'\n",
    "start_date = \"2022-01-10\"\n",
    "end_date = \"2025-04-12\" #added the new dates for testing purposes\n",
    "\n",
    "# Define the new start date (29 days after original start date)\n",
    "adjusted_start_date = pd.to_datetime(start_date) + pd.Timedelta(days=29)\n",
    "\n",
    "# Create directories\n",
    "raw_data_dir = 'New_HistoricalData'\n",
    "processed_data_dir = 'New_ProcessedHistoricalData'\n",
    "adjusted_data_dir = 'New_AdjustedHistoricalData'\n",
    "pickle_dir = 'New_PickleData'\n",
    "os.makedirs(raw_data_dir, exist_ok=True)\n",
    "os.makedirs(processed_data_dir, exist_ok=True)\n",
    "os.makedirs(adjusted_data_dir, exist_ok=True)\n",
    "os.makedirs(pickle_dir, exist_ok=True)\n",
    "\n",
    "# Dictionary to store stock data\n",
    "stock_data_dict = {}\n",
    "sector_data_dict = {}\n",
    "\n",
    "# Download stock data\n",
    "for ticker in tqdm(nifty500_tickers[0:500], desc=\"Downloading Stock Data\"):\n",
    "    try:\n",
    "        ticker_obj = yf.Ticker(ticker)\n",
    "        company_info = ticker_obj.info\n",
    "        company_history = ticker_obj.history(start=start_date, end=end_date)\n",
    "        company_history.index = company_history.index.tz_localize(None)\n",
    "\n",
    "        # Drop unwanted columns\n",
    "        company_history.drop(columns=['Dividends', 'Stock Splits'], inplace=True, errors='ignore')\n",
    "\n",
    "        # Remove weeks with no data\n",
    "        company_history['Week'] = company_history.index.to_period('W')\n",
    "        valid_weeks = company_history.groupby('Week')['Close'].count() > 0\n",
    "        company_history = company_history[company_history['Week'].isin(valid_weeks[valid_weeks].index)]\n",
    "        company_history.drop(columns=['Week'], inplace=True)\n",
    "\n",
    "        # Get sector information\n",
    "        sector = company_info.get(\"sector\", \"Unknown\")\n",
    "        company_history.insert(1, \"Sector\", sector)\n",
    "\n",
    "        # Save cleaned raw data\n",
    "        file_name = ticker.replace('.NS', '') + '.csv'\n",
    "        company_history.to_csv(os.path.join(raw_data_dir, file_name), index=True)\n",
    "\n",
    "        # Store in dictionary\n",
    "        stock_data_dict[ticker] = company_history\n",
    "        sector_data_dict[ticker] = sector\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't fetch data for {ticker}: {e}\")\n",
    "\n",
    "# Save raw data dictionary as pickle\n",
    "with open(os.path.join(pickle_dir, 'raw_stock_data.pkl'), 'wb') as f:\n",
    "    pickle.dump(stock_data_dict, f)\n",
    "\n",
    "with open(os.path.join(pickle_dir, 'sector_data.pkl'), 'wb') as f:\n",
    "    pickle.dump(sector_data_dict, f)\n",
    "\n",
    "# Function to calculate RSI\n",
    "def calculate_rsi(data, period=14):\n",
    "    delta = data['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
    "    rs = gain / loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# Function to calculate MACD\n",
    "def calculate_macd(data, short_window=12, long_window=26, signal_window=9):\n",
    "    short_ema = data['Close'].ewm(span=short_window, adjust=False).mean()\n",
    "    long_ema = data['Close'].ewm(span=long_window, adjust=False).mean()\n",
    "    macd = short_ema - long_ema\n",
    "    signal = macd.ewm(span=signal_window, adjust=False).mean()\n",
    "    return macd, signal\n",
    "\n",
    "# Process stock data\n",
    "def process_stock_data(stock_data):\n",
    "    stock_data['normalized_close'] = StandardScaler().fit_transform(stock_data[['Close']])\n",
    "    stock_data['return_ratio'] = stock_data['Close'].pct_change()\n",
    "    for window in [5, 10, 15, 20, 25, 30]:\n",
    "        stock_data[f'MA_{window}'] = stock_data['Close'].rolling(window=window, min_periods=1).mean()\n",
    "    for col in ['Open', 'High', 'Low']:\n",
    "        stock_data[f'{col}_pct_change'] = stock_data[col] / stock_data['Close'] - 1\n",
    "    stock_data['RSI'] = calculate_rsi(stock_data)\n",
    "    stock_data['MACD'], stock_data['MACD_signal'] = calculate_macd(stock_data)\n",
    "    return stock_data\n",
    "\n",
    "# Dictionary to store processed data\n",
    "processed_data_dict = {}\n",
    "adjusted_data_dict = {}\n",
    "\n",
    "# Process and save data\n",
    "for file_name in tqdm(os.listdir(raw_data_dir), desc=\"Processing Stock Data\"):\n",
    "    if file_name.endswith('.csv'):\n",
    "        symbol = file_name.split('.')[0]\n",
    "        stock_data = pd.read_csv(os.path.join(raw_data_dir, file_name), index_col=0, parse_dates=True)\n",
    "        processed_data = process_stock_data(stock_data)\n",
    "        processed_data.to_csv(os.path.join(processed_data_dir, file_name), index=True)\n",
    "\n",
    "        # Store in dictionary\n",
    "        processed_data_dict[symbol] = processed_data\n",
    "\n",
    "        # Extract data starting from the 29th day\n",
    "        adjusted_data = processed_data[processed_data.index >= adjusted_start_date]\n",
    "        if not adjusted_data.empty:\n",
    "            adjusted_data.to_csv(os.path.join(adjusted_data_dir, file_name), index=True)\n",
    "            adjusted_data_dict[symbol] = adjusted_data\n",
    "\n",
    "# Save processed and adjusted data dictionaries as pickle\n",
    "with open(os.path.join(pickle_dir, 'processed_stock_data.pkl'), 'wb') as f:\n",
    "    pickle.dump(processed_data_dict, f)\n",
    "\n",
    "with open(os.path.join(pickle_dir, 'adjusted_stock_data.pkl'), 'wb') as f:\n",
    "    pickle.dump(adjusted_data_dict, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f07ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Directories\n",
    "input_dir = 'New_AdjustedHistoricalData'\n",
    "output_dir = 'New_NormalizedProcessedData'\n",
    "pickle_dir = 'New_PickleData'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(pickle_dir, exist_ok=True)\n",
    "pickle_file = os.path.join(pickle_dir, 'normalized_stock_data.pkl')\n",
    "\n",
    "# Dictionary to store normalized data\n",
    "normalized_data_dict = {}\n",
    "\n",
    "# Process each file in input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure Date and Sector columns exist\n",
    "        if 'Date' not in df.columns:\n",
    "            df['Date'] = df.index  # Add Date as a column if missing\n",
    "        if 'Sector' not in df.columns:\n",
    "            df['Sector'] = \"Unknown\"  # Fallback if sector info is missing\n",
    "\n",
    "        # Identify column groups\n",
    "        non_numeric_cols = ['Date', 'Sector']\n",
    "        minmax_cols = ['MA_5', 'MA_10', 'MA_15', 'MA_20', 'MA_25', 'MA_30', 'RSI', 'MACD', 'MACD_signal', 'Open_pct_change', 'High_pct_change', 'Low_pct_change']  # MinMaxScaler\n",
    "        volume_cols = ['Volume']  # Log + MinMaxScaler\n",
    "\n",
    "        # Apply MinMaxScaler (0-1 normalization) to moving averages, momentum indicators, and percentage changes\n",
    "        scaler_minmax = MinMaxScaler()\n",
    "        df[minmax_cols] = scaler_minmax.fit_transform(df[minmax_cols])\n",
    "\n",
    "        # Apply Log Scaling + MinMaxScaler to Volume\n",
    "        df[volume_cols] = np.log1p(df[volume_cols])  # Log transform\n",
    "        df[volume_cols] = scaler_minmax.fit_transform(df[volume_cols])  # MinMax scale\n",
    "\n",
    "        # Compute normalized close price\n",
    "        df['normalized_close'] = (df['Close'] - df['Close'].min()) / (df['Close'].max() - df['Close'].min())\n",
    "\n",
    "        # Compute return ratio\n",
    "        df['return_ratio'] = df['Close'].pct_change().fillna(0)\n",
    "\n",
    "        # Save the normalized data\n",
    "        output_filename = f\"{filename.replace('.csv', '_norm.csv')}\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        df.to_csv(output_path, index=False)\n",
    "\n",
    "        # Store in dictionary\n",
    "        normalized_data_dict[filename] = df\n",
    "\n",
    "        #print(f\"✅ Normalization complete! Saved: {output_filename}\")\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(normalized_data_dict, f)\n",
    "\n",
    "print(\"✅ All normalized data saved as a pickle file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e56f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# Enhanced Configuration\n",
    "class Config:\n",
    "    data_dir = \"New_NormalizedProcessedData\"\n",
    "    output_dir = \"New_ShortTerm_Stock_Embeddings\"\n",
    "    seq_length = 30\n",
    "    hidden_dim = 128\n",
    "    num_layers = 2\n",
    "    batch_size = 128\n",
    "    epochs = 50\n",
    "    initial_lr = 0.001\n",
    "    weight_decay = 1e-5  # L2 regularization\n",
    "    dropout = 0.25\n",
    "    feature_columns = [\n",
    "        \"MA_5\", \"MA_10\", \"MA_15\", \"MA_20\", \"MA_25\", \"MA_30\",\n",
    "        \"RSI\", \"MACD\", \"MACD_signal\", \"Volume\",\n",
    "        \"Open_pct_change\", \"High_pct_change\", \"Low_pct_change\"\n",
    "    ]\n",
    "    target_column = \"return_ratio\"\n",
    "    train_cutoff_date = \"2024-01-11\"  # All data before this is training\n",
    "\n",
    "# Volatility-Aware Attentive LSTM \n",
    "class VolatilityAwareAttentiveLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.25):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Standard attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=2, batch_first=True)\n",
    "        \n",
    "        # Volatility awareness components\n",
    "        self.volatility_detector = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()  # Normalize volatility score between 0-1\n",
    "        )\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),  # Layer normalization for better stability\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_len, input_dim]\n",
    "        lstm_out, _ = self.lstm(x)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # Calculate volatility weights\n",
    "        batch_size, seq_len, _ = lstm_out.shape\n",
    "        volatility_scores = torch.zeros(batch_size, seq_len).to(x.device)\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            volatility_scores[:, i] = self.volatility_detector(lstm_out[:, i, :]).squeeze(-1)\n",
    "        \n",
    "        # Apply standard attention\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Apply volatility weighting to attention output\n",
    "        weighted_out = attn_out * volatility_scores.unsqueeze(-1)\n",
    "        \n",
    "        # Get context vector by weighted \n",
    "        context = torch.sum(weighted_out, dim=1) / (torch.sum(volatility_scores, dim=1, keepdim=True) + 1e-8)\n",
    "        \n",
    "        # Prediction\n",
    "        prediction = self.fc_layers(context)\n",
    "        \n",
    "        return prediction, context\n",
    "\n",
    "# Training function with enhanced evaluation\n",
    "def train_and_evaluate(stock_name, df, config):\n",
    "    # Prepare data\n",
    "    data = df[config.feature_columns].values\n",
    "    targets = df[config.target_column].values\n",
    "    dates = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    def create_sequences(data, targets, seq_length):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            X.append(data[i:i + seq_length])\n",
    "            y.append(targets[i + seq_length])\n",
    "        return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "    \n",
    "    X, y = create_sequences(data, targets, config.seq_length)\n",
    "    seq_dates = dates[config.seq_length:].reset_index(drop=True)\n",
    "    \n",
    "    # Time-based train-test split\n",
    "    cutoff_date = pd.to_datetime(config.train_cutoff_date)\n",
    "    train_indices = seq_dates[seq_dates < cutoff_date].index.tolist()\n",
    "    test_indices = seq_dates[seq_dates >= cutoff_date].index.tolist()\n",
    "    \n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "    dates_test = seq_dates.iloc[test_indices]\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_train = torch.tensor(X_train).float()\n",
    "    X_test = torch.tensor(X_test).float()\n",
    "    y_train = torch.tensor(y_train).float().unsqueeze(1)\n",
    "    y_test = torch.tensor(y_test).float().unsqueeze(1)\n",
    "    \n",
    "    # Check if CUDA is available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Model setup\n",
    "    model = VolatilityAwareAttentiveLSTM(\n",
    "        len(config.feature_columns), \n",
    "        config.hidden_dim, \n",
    "        config.num_layers,\n",
    "        config.dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=config.initial_lr,\n",
    "        weight_decay=config.weight_decay  # L2 regularization\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5,\n",
    "        patience=5, \n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training loop with validation\n",
    "    train_losses = []\n",
    "    val_indices = train_indices[-len(train_indices)//5:]  # Use last 20% of training data for validation\n",
    "    train_indices = train_indices[:-len(train_indices)//5]\n",
    "    \n",
    "    X_val = torch.tensor(X[val_indices]).float().to(device)\n",
    "    y_val = torch.tensor(y[val_indices]).float().unsqueeze(1).to(device)\n",
    "    \n",
    "    # Move training data to device\n",
    "    X_train_final = torch.tensor(X[train_indices]).float().to(device)\n",
    "    y_train_final = torch.tensor(y[train_indices]).float().unsqueeze(1).to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in tqdm(range(config.epochs), desc=f\"Training {stock_name}\"):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        batch_indices = torch.randperm(len(X_train_final))\n",
    "        \n",
    "        for i in range(0, len(X_train_final), config.batch_size):\n",
    "            batch_idx = batch_indices[i:i + config.batch_size]\n",
    "            X_batch = X_train_final[batch_idx]\n",
    "            y_batch = y_train_final[batch_idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds, _ = model(X_batch)\n",
    "            loss = criterion(preds, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * len(X_batch)\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(X_train_final)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_preds, _ = model(X_val)\n",
    "            val_loss = criterion(val_preds, y_val).item()\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Final test evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_preds, test_embs = model(X_test)\n",
    "        \n",
    "        # Move to CPU for numpy operations\n",
    "        test_preds_np = test_preds.cpu().numpy()\n",
    "        y_test_np = y_test.cpu().numpy()\n",
    "        test_embs_np = test_embs.cpu().numpy()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        test_mse = mean_squared_error(y_test_np, test_preds_np)\n",
    "        test_rmse = np.sqrt(test_mse)\n",
    "        test_mae = mean_absolute_error(y_test_np, test_preds_np)\n",
    "        test_r2 = r2_score(y_test_np, test_preds_np)\n",
    "        \n",
    "        # Directional accuracy\n",
    "        actual_direction = np.sign(y_test_np)\n",
    "        pred_direction = np.sign(test_preds_np)\n",
    "        directional_accuracy = np.mean(actual_direction == pred_direction)\n",
    "        \n",
    "        # Information coefficient (IC) - correlation between predictions and actuals\n",
    "        ic = np.corrcoef(test_preds_np.flatten(), y_test_np.flatten())[0, 1]\n",
    "        \n",
    "        # Maximum drawdown calculation\n",
    "        # Assuming predictions are return ratios, calculate cumulative returns\n",
    "        cumulative_actual = np.cumprod(1 + y_test_np.flatten())\n",
    "        peak = np.maximum.accumulate(cumulative_actual)\n",
    "        drawdown = (peak - cumulative_actual) / peak\n",
    "        max_drawdown = np.max(drawdown)\n",
    "    \n",
    "    # Save results\n",
    "    save_results(\n",
    "        stock_name, train_losses, \n",
    "        {\n",
    "            'MSE': test_mse,\n",
    "            'RMSE': test_rmse,\n",
    "            'MAE': test_mae,\n",
    "            'R²': test_r2,\n",
    "            'Directional_Accuracy': directional_accuracy,\n",
    "            'Information_Coefficient': ic,\n",
    "            'Max_Drawdown': max_drawdown\n",
    "        },\n",
    "        test_embs_np, df.iloc[test_indices], dates_test, config\n",
    "    )\n",
    "    \n",
    "    return model, test_embs_np, dates_test, test_preds_np, y_test_np\n",
    "\n",
    "def save_results(stock_name, train_losses, metrics, \n",
    "                 test_embs, test_df, test_dates, config):\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss (MSE)')\n",
    "    plt.title(f\"Training Curve for {stock_name}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"{config.output_dir}/{stock_name}_train_loss.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Save test metrics\n",
    "    with open(f\"{config.output_dir}/{stock_name}_metrics.txt\", \"w\") as f:\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            f.write(f\"{metric_name}: {metric_value:.4f}\\n\")\n",
    "    \n",
    "    # Save test embeddings with date and sector information\n",
    "    embedding_df = pd.DataFrame({\n",
    "        'date': test_dates,\n",
    "        'stock': stock_name,\n",
    "        'sector': test_df['Sector'].iloc[0] if 'Sector' in test_df.columns else 'Unknown',\n",
    "        **{f'emb_{i}': test_embs[:, i] for i in range(test_embs.shape[1])}\n",
    "    })\n",
    "    \n",
    "    # This is now the primary output - the embeddings CSV file\n",
    "    embedding_df.to_csv(f\"{config.output_dir}/{stock_name}_embeddings.csv\", index=False)\n",
    "\n",
    "    # Create visualization of embeddings over time (2D PCA)\n",
    "    if test_embs.shape[0] > 2:  # Need at least 2 samples for PCA\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        emb_2d = pca.fit_transform(test_embs)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sc = plt.scatter(emb_2d[:, 0], emb_2d[:, 1], \n",
    "                         c=range(len(emb_2d)), cmap='viridis', alpha=0.7)\n",
    "        plt.colorbar(sc, label='Time Sequence')\n",
    "        plt.title(f\"Temporal Embedding Evolution for {stock_name}\")\n",
    "        plt.xlabel(f\"PCA Component 1 (Var: {pca.explained_variance_ratio_[0]:.2f})\")\n",
    "        plt.ylabel(f\"PCA Component 2 (Var: {pca.explained_variance_ratio_[1]:.2f})\")\n",
    "        plt.savefig(f\"{config.output_dir}/{stock_name}_embedding_evolution.png\")\n",
    "        plt.close()\n",
    "\n",
    "# Function to visualize prediction performance\n",
    "def visualize_predictions(stock_name, dates, y_true, y_pred, config):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(dates, y_true, label='Actual Returns', color='blue', alpha=0.7)\n",
    "    plt.plot(dates, y_pred, label='Predicted Returns', color='red', alpha=0.7)\n",
    "    plt.fill_between(dates, y_true.flatten(), y_pred.flatten(), \n",
    "                     color='gray', alpha=0.3, label='Prediction Error')\n",
    "    \n",
    "    plt.title(f\"Return Prediction Performance for {stock_name}\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Return Ratio\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add directional accuracy\n",
    "    correct_dir = np.sign(y_true) == np.sign(y_pred)\n",
    "    dir_acc = np.mean(correct_dir) * 100\n",
    "    plt.figtext(0.15, 0.85, f\"Directional Accuracy: {dir_acc:.2f}%\",\n",
    "                bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{config.output_dir}/{stock_name}_prediction_performance.png\")\n",
    "    plt.close()\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    config = Config()\n",
    "    os.makedirs(config.output_dir, exist_ok=True)\n",
    "    \n",
    "    all_metrics = []\n",
    "    all_embeddings = {}\n",
    "    \n",
    "    for file_name in tqdm(os.listdir(config.data_dir), desc=\"Processing Stocks\"):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            stock_name = file_name.replace(\"_norm.csv\", \"\")\n",
    "            df = pd.read_csv(os.path.join(config.data_dir, file_name))\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            \n",
    "            try:\n",
    "                model, test_embs, test_dates, test_preds, y_test = train_and_evaluate(stock_name, df, config)\n",
    "                \n",
    "                # Visualize predictions\n",
    "                visualize_predictions(stock_name, test_dates, y_test, test_preds, config)\n",
    "                \n",
    "                # Store metrics for comparative analysis\n",
    "                test_mse = mean_squared_error(y_test, test_preds)\n",
    "                test_rmse = np.sqrt(test_mse)\n",
    "                test_mae = mean_absolute_error(y_test, test_preds)\n",
    "                test_r2 = r2_score(y_test, test_preds)\n",
    "                \n",
    "                # Directional accuracy\n",
    "                actual_direction = np.sign(y_test)\n",
    "                pred_direction = np.sign(test_preds)\n",
    "                directional_accuracy = np.mean(actual_direction == pred_direction)\n",
    "                \n",
    "                all_metrics.append({\n",
    "                    'stock': stock_name,\n",
    "                    'sector': df['Sector'].iloc[0] if 'Sector' in df.columns else 'Unknown',\n",
    "                    'MSE': test_mse,\n",
    "                    'RMSE': test_rmse,\n",
    "                    'MAE': test_mae,\n",
    "                    'R²': test_r2,\n",
    "                    'Directional_Accuracy': directional_accuracy,\n",
    "                })\n",
    "                \n",
    "                # Store embeddings for later cross-stock analysis\n",
    "                all_embeddings[stock_name] = {\n",
    "                    'embeddings': test_embs,\n",
    "                    'dates': test_dates,\n",
    "                    'sector': df['Sector'].iloc[0] if 'Sector' in df.columns else 'Unknown'\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed on {stock_name}: {str(e)}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6928e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# ========== CONFIGURATION ==========\n",
    "embedding_dir = \"New_ShortTerm_Stock_Embeddings\"\n",
    "output_dir = \"New_Graphs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "intra_sector_graph_path = os.path.join(output_dir, \"Intra_Sector_Graph.csv\")\n",
    "sector_graphs_dir = os.path.join(output_dir, \"Sector_Graphs\")\n",
    "os.makedirs(sector_graphs_dir, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ========== DATA LOADING ==========\n",
    "embedding_files = [f for f in os.listdir(embedding_dir) if f.endswith(\".csv\")]\n",
    "stock_embeddings = {}\n",
    "stock_sectors = {}\n",
    "embedding_size = None\n",
    "\n",
    "# First pass: Identify all sectors and handle ISEC\n",
    "print(\"\\nScanning sectors and processing ISEC...\")\n",
    "for file in embedding_files:\n",
    "    try:\n",
    "        stock_name = file.split(\"_\")[0]\n",
    "        if stock_name == \"ISEC\":\n",
    "            stock_sectors[\"ISEC\"] = \"Financial Services\"\n",
    "            print(\"✅ Manually set ISEC sector to Financial Services\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error scanning {file}: {str(e)}\")\n",
    "\n",
    "# Second pass: Load embeddings\n",
    "print(\"\\nLoading stock embeddings...\")\n",
    "for file in embedding_files:\n",
    "    try:\n",
    "        stock_name = file.split(\"_\")[0]\n",
    "        if stock_name == \"ISEC\":\n",
    "            continue  # Already handled\n",
    "            \n",
    "        df = pd.read_csv(os.path.join(embedding_dir, file))\n",
    "        \n",
    "        # Process embedding columns\n",
    "        emb_cols = [col for col in df.columns if col.startswith('emb_')]\n",
    "        if not emb_cols:\n",
    "            print(f\"⚠️ {file}: No embedding columns found\")\n",
    "            continue\n",
    "            \n",
    "        if embedding_size is None:\n",
    "            embedding_size = len(emb_cols)\n",
    "            print(f\"Detected embedding dimension: {embedding_size}\")\n",
    "        elif len(emb_cols) != embedding_size:\n",
    "            print(f\"⚠️ {file}: Dimension mismatch ({len(emb_cols)} vs {embedding_size})\")\n",
    "            continue\n",
    "            \n",
    "        # Numeric conversion and validation\n",
    "        df[emb_cols] = df[emb_cols].apply(pd.to_numeric, errors='coerce')\n",
    "        if df[emb_cols].isnull().values.any():\n",
    "            print(f\"⚠️ {file}: Contains invalid numeric values\")\n",
    "            continue\n",
    "            \n",
    "        sector = df.iloc[0, 2]\n",
    "        stock_sectors[stock_name] = sector\n",
    "        stock_embeddings[stock_name] = df[emb_cols].to_numpy()\n",
    "        \n",
    "        print(f\"✓ Loaded {stock_name} ({sector}) with {len(df)} days\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to process {file}: {str(e)}\")\n",
    "\n",
    "# ========== GRAPH CONSTRUCTION ==========\n",
    "print(\"\\nBuilding intra-sector graph...\")\n",
    "G_intra = nx.Graph()\n",
    "\n",
    "# Create comprehensive color palette\n",
    "all_sectors = sorted(set(stock_sectors.values()))\n",
    "base_colors = list(mcolors.TABLEAU_COLORS.values()) + list(mcolors.XKCD_COLORS.values())\n",
    "sector_colors = {sector: base_colors[i % len(base_colors)] for i, sector in enumerate(all_sectors)}\n",
    "\n",
    "# Add sector nodes\n",
    "for sector in all_sectors:\n",
    "    G_intra.add_node(sector, type=\"sector\", color=sector_colors[sector])\n",
    "\n",
    "# Add stock nodes and sector edges\n",
    "for stock, embeddings in stock_embeddings.items():\n",
    "    sector = stock_sectors[stock]\n",
    "    try:\n",
    "        avg_embedding = np.mean(embeddings, axis=0)\n",
    "        if np.isnan(avg_embedding).any():\n",
    "            print(f\"⚠️ {stock}: NaN in averaged embeddings\")\n",
    "            continue\n",
    "            \n",
    "        G_intra.add_node(stock, sector=sector, embedding=avg_embedding)\n",
    "        G_intra.add_edge(stock, sector, weight=1.0)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to add {stock}: {str(e)}\")\n",
    "\n",
    "# Ensure ISEC is properly connected\n",
    "if \"ISEC\" in stock_sectors and \"ISEC\" not in G_intra:\n",
    "    sector = stock_sectors[\"ISEC\"]\n",
    "    dummy_embedding = np.zeros(embedding_size) if embedding_size else np.zeros(128)\n",
    "    G_intra.add_node(\"ISEC\", sector=sector, embedding=dummy_embedding)\n",
    "    G_intra.add_edge(\"ISEC\", sector, weight=1.0)\n",
    "    print(\"✅ Force-added ISEC to graph\")\n",
    "\n",
    "# ========== SIMILARITY CALCULATION ==========\n",
    "print(\"\\nCalculating cosine similarities...\")\n",
    "stock_list = [s for s in stock_embeddings.keys() if s in G_intra.nodes]\n",
    "if not stock_list:\n",
    "    raise ValueError(\"No valid stocks found for similarity calculation\")\n",
    "\n",
    "try:\n",
    "    embedding_matrix = np.vstack([G_intra.nodes[s]['embedding'] for s in stock_list])\n",
    "    if torch.cuda.is_available():\n",
    "        tensor = torch.tensor(embedding_matrix, device=device)\n",
    "        sim_matrix = torch.nn.functional.cosine_similarity(\n",
    "            tensor.unsqueeze(1), tensor.unsqueeze(0), dim=-1\n",
    "        ).cpu().numpy()\n",
    "    else:\n",
    "        sim_matrix = cosine_similarity(embedding_matrix)\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Similarity calculation failed: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Add similarity edges\n",
    "threshold = 0.8\n",
    "print(f\"\\nAdding edges with similarity > {threshold}\")\n",
    "for i, j in combinations(range(len(stock_list)), 2):\n",
    "    s1, s2 = stock_list[i], stock_list[j]\n",
    "    sim = sim_matrix[i, j]\n",
    "    same_sector = G_intra.nodes[s1]['sector'] == G_intra.nodes[s2]['sector']\n",
    "    if same_sector and sim > threshold:\n",
    "        G_intra.add_edge(s1, s2, weight=sim)\n",
    "\n",
    "# ========== OUTPUT GENERATION ==========\n",
    "def save_edges(graph, csv_path):\n",
    "    edges = [(u, v, d['weight']) for u, v, d in graph.edges(data=True)]\n",
    "    df = pd.DataFrame(edges, columns=[\"source\", \"target\", \"weight\"])\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"✅ Saved {len(edges)} edges to {csv_path}\")\n",
    "\n",
    "print(\"\\nSaving intra-sector graph:\")\n",
    "save_edges(G_intra, intra_sector_graph_path)\n",
    "\n",
    "# ========== VISUALIZATION ==========\n",
    "print(\"\\nGenerating intra-sector visualizations...\")\n",
    "def plot_enhanced_graph(graph, title, filename):\n",
    "    plt.figure(figsize=(20, 16))\n",
    "    pos = nx.spring_layout(graph, k=0.15, iterations=50)\n",
    "    \n",
    "    # Get node colors safely\n",
    "    node_colors = []\n",
    "    for node in graph.nodes():\n",
    "        if 'color' in graph.nodes[node]:\n",
    "            node_colors.append(graph.nodes[node]['color'])\n",
    "        else:\n",
    "            sector = graph.nodes[node].get('sector')\n",
    "            node_colors.append(sector_colors.get(sector, 'gray'))\n",
    "    \n",
    "    nx.draw_networkx_edges(\n",
    "        graph, pos,\n",
    "        width=[2*d['weight'] for _,_,d in graph.edges(data=True)],\n",
    "        alpha=0.3, edge_color='gray'\n",
    "    )\n",
    "    nx.draw_networkx_nodes(\n",
    "        graph, pos,\n",
    "        node_size=[1200 if graph.nodes[node].get('type') == 'sector' else 600 \n",
    "                  for node in graph.nodes()],\n",
    "        node_color=node_colors,\n",
    "        alpha=0.9\n",
    "    )\n",
    "    \n",
    "    # Label only sectors\n",
    "    nx.draw_networkx_labels(\n",
    "        graph, pos,\n",
    "        labels={n: n for n in graph.nodes() if graph.nodes[n].get('type') == 'sector'},\n",
    "        font_size=14,\n",
    "        font_weight='bold'\n",
    "    )\n",
    "    \n",
    "    plt.title(f\"{title}\\nNodes: {graph.number_of_nodes()} | Edges: {graph.number_of_edges()}\", fontsize=16)\n",
    "    plt.savefig(os.path.join(output_dir, filename), bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "plot_enhanced_graph(G_intra, \"Intra-Sector Network\", \"Intra_Sector_Graph_enhanced.png\")\n",
    "\n",
    "# Sector-specific plots\n",
    "print(\"\\nGenerating sector-specific plots:\")\n",
    "for sector in all_sectors:\n",
    "    plt.figure(figsize=(16, 14))\n",
    "    nodes = [n for n in G_intra.nodes if G_intra.nodes[n].get('sector') == sector or n == sector]\n",
    "    if len(nodes) <= 1:\n",
    "        continue\n",
    "        \n",
    "    subgraph = G_intra.subgraph(nodes)\n",
    "    pos = nx.spring_layout(subgraph, k=0.3)\n",
    "    \n",
    "    nx.draw(\n",
    "        subgraph, pos,\n",
    "        node_color=sector_colors[sector],\n",
    "        node_size=800,\n",
    "        with_labels=True,\n",
    "        font_size=10,\n",
    "        edge_color=\"gray\",\n",
    "        alpha=0.9,\n",
    "        width=[2*d['weight'] for _,_,d in subgraph.edges(data=True)]\n",
    "    )\n",
    "    plt.title(f\"{sector} Sector\\n{len(nodes)-1} stocks\", fontsize=14)\n",
    "    plt.savefig(\n",
    "        os.path.join(sector_graphs_dir, f\"{sector}_Graph_enhanced.png\"),\n",
    "        bbox_inches='tight', dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "    print(f\"✓ Generated {sector} sector plot\")\n",
    "\n",
    "# ========== FINAL OUTPUT ==========\n",
    "print(\"\\n✅ Processing completed successfully\")\n",
    "print(f\"Final counts - Intra-Sector Graph: {G_intra.number_of_nodes()} nodes, {G_intra.number_of_edges()} edges\")\n",
    "\n",
    "# Verify ISEC presence\n",
    "if \"ISEC\" in stock_sectors:\n",
    "    print(\"\\nISEC verification:\")\n",
    "    if \"ISEC\" in G_intra.nodes():\n",
    "        print(\"✓ ISEC present in intra-sector graph\")\n",
    "        print(f\"Connections: {list(G_intra.edges('ISEC'))}\")\n",
    "    else:\n",
    "        print(\"⚠️ ISEC missing from intra-sector graph\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a4a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class StockGraphBuilder:\n",
    "    def __init__(self, embedding_dir=\"New_ShortTerm_Stock_Embeddings\"):\n",
    "        self.embedding_dir = embedding_dir\n",
    "        self.output_dir = \"New_Graphs\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def build_graphs(self):\n",
    "        \"\"\"Complete graph construction pipeline\"\"\"\n",
    "        print(\"1. Loading embeddings...\")\n",
    "        stock_data = self._load_with_isec_handling()\n",
    "        \n",
    "        print(\"\\n2. Computing similarities...\")\n",
    "        similarity_matrix = self._compute_similarities(stock_data)\n",
    "        \n",
    "        print(\"\\n3. Building graph...\")\n",
    "        G = self._build_with_dynamic_thresholds(stock_data, similarity_matrix)\n",
    "        \n",
    "        print(\"\\n4. Analyzing results...\")\n",
    "        self._enhanced_analysis(G)\n",
    "        return G\n",
    "\n",
    "    def _load_with_isec_handling(self):\n",
    "        \"\"\"Special handling for ISEC while validating others\"\"\"\n",
    "        embeddings = {}\n",
    "        valid_stocks = 0\n",
    "        \n",
    "        for file in tqdm(os.listdir(self.embedding_dir)):\n",
    "            if not file.endswith(\"_embeddings.csv\"):\n",
    "                continue\n",
    "                \n",
    "            stock_name = file.split(\"_\")[0]\n",
    "            df = pd.read_csv(os.path.join(self.embedding_dir, file))\n",
    "            \n",
    "            # ISEC special case\n",
    "            if stock_name == \"ISEC\":\n",
    "                sector = \"Financial Services\"\n",
    "                print(\"✅ Manually assigned sector to ISEC\")\n",
    "            elif 'sector' in df.columns:\n",
    "                sector = df['sector'].iloc[0]\n",
    "            else:\n",
    "                print(f\"⚠️ Skipping {stock_name}: No sector data\")\n",
    "                continue\n",
    "                \n",
    "            # Validate embeddings\n",
    "            emb = df.filter(regex='emb_').values\n",
    "            if np.isnan(emb).any() or np.all(emb == 0):\n",
    "                print(f\"⚠️ Skipping {stock_name}: Invalid embeddings\")\n",
    "                continue\n",
    "                \n",
    "            embeddings[stock_name] = {\n",
    "                'sector': sector,\n",
    "                'embedding': np.mean(emb, axis=0)\n",
    "            }\n",
    "            valid_stocks += 1\n",
    "            \n",
    "        print(f\"\\nLoaded {valid_stocks} valid stocks (including ISEC)\")\n",
    "        return embeddings\n",
    "\n",
    "    def _compute_similarities(self, stock_data):\n",
    "        \"\"\"Compute similarity matrix with diagnostics\"\"\"\n",
    "        stock_names = list(stock_data.keys())\n",
    "        embeddings = np.array([stock_data[name]['embedding'] for name in stock_names])\n",
    "        \n",
    "        print(f\"Embedding matrix shape: {embeddings.shape}\")\n",
    "        print(f\"NaN values: {np.isnan(embeddings).sum()}\")\n",
    "        print(f\"Zero embeddings: {np.all(embeddings == 0, axis=1).sum()}\")\n",
    "        \n",
    "        similarity = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Calculate similarity statistics\n",
    "        print(f\"\\nSimilarity statistics:\")\n",
    "        print(f\"Min: {similarity.min():.3f}\")\n",
    "        print(f\"Max: {similarity.max():.3f}\")\n",
    "        print(f\"Mean: {similarity.mean():.3f}\")\n",
    "        print(f\"Median: {np.median(similarity):.3f}\")\n",
    "        \n",
    "        # Plot similarity distribution\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.hist(similarity.flatten(), bins=50)\n",
    "        plt.title(\"Cosine Similarity Distribution\")\n",
    "        plt.savefig(os.path.join(self.output_dir, \"similarity_distribution.png\"))\n",
    "        plt.close()\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "    def _build_with_dynamic_thresholds(self, stock_data, similarity_matrix):\n",
    "        \"\"\"Build graph with dynamic percentile-based thresholds\"\"\"\n",
    "        G = nx.Graph()\n",
    "        stock_names = list(stock_data.keys())\n",
    "        \n",
    "        # Calculate dynamic thresholds\n",
    "        intra_sims = []\n",
    "        inter_sims = []\n",
    "        \n",
    "        for i, j in combinations(range(len(stock_names)), 2):\n",
    "            s1, s2 = stock_names[i], stock_names[j]\n",
    "            if stock_data[s1]['sector'] == stock_data[s2]['sector']:\n",
    "                intra_sims.append(similarity_matrix[i, j])\n",
    "            else:\n",
    "                inter_sims.append(similarity_matrix[i, j])\n",
    "        \n",
    "        intra_thresh = np.percentile(intra_sims, 75) if intra_sims else 0\n",
    "        inter_thresh = np.percentile(inter_sims, 80) if inter_sims else 0\n",
    "        \n",
    "        print(f\"\\nUsing dynamic thresholds:\")\n",
    "        print(f\"- Intra-sector (75th percentile): {intra_thresh:.3f}\")\n",
    "        print(f\"- Inter-sector (80th percentile): {inter_thresh:.3f}\")\n",
    "        \n",
    "        # Add nodes\n",
    "        for name in stock_names:\n",
    "            G.add_node(name, sector=stock_data[name]['sector'])\n",
    "        \n",
    "        # Track connections\n",
    "        intra_edges = 0\n",
    "        inter_edges = 0\n",
    "        \n",
    "        # Connect top similarities within sectors\n",
    "        for sector in set(stock_data[n]['sector'] for n in stock_names):\n",
    "            sector_nodes = [n for n in stock_names if stock_data[n]['sector'] == sector]\n",
    "            sector_indices = [stock_names.index(n) for n in sector_nodes]\n",
    "            \n",
    "            # Create complete similarity matrix for sector\n",
    "            sector_sim = similarity_matrix[np.ix_(sector_indices, sector_indices)]\n",
    "            np.fill_diagonal(sector_sim, 0)  # Remove self-similarities\n",
    "            \n",
    "            # Connect top 20% of possible edges within sector\n",
    "            k = int(0.2 * len(sector_nodes) * (len(sector_nodes) - 1) // 2)\n",
    "            if k > 0:\n",
    "                flat_indices = np.argpartition(sector_sim.flatten(), -k)[-k:]\n",
    "                rows, cols = np.unravel_index(flat_indices, sector_sim.shape)\n",
    "                for r, c in zip(rows, cols):\n",
    "                    n1, n2 = sector_nodes[r], sector_nodes[c]\n",
    "                    G.add_edge(n1, n2, weight=sector_sim[r,c], type='intra')\n",
    "                    intra_edges += 1\n",
    "        \n",
    "        # Connect across sectors\n",
    "        for i, j in combinations(range(len(stock_names)), 2):\n",
    "            s1, s2 = stock_names[i], stock_names[j]\n",
    "            sim = similarity_matrix[i, j]\n",
    "            \n",
    "            if stock_data[s1]['sector'] != stock_data[s2]['sector'] and sim > inter_thresh:\n",
    "                G.add_edge(s1, s2, weight=sim, type='inter')\n",
    "                inter_edges += 1\n",
    "        \n",
    "        print(f\"\\nCreated {intra_edges} intra-sector and {inter_edges} inter-sector edges\")\n",
    "        return G\n",
    "\n",
    "    def _enhanced_analysis(self, G):\n",
    "        \"\"\"Comprehensive graph analysis with community detection\"\"\"\n",
    "        print(\"\\nGraph Statistics:\")\n",
    "        print(f\"- Nodes: {len(G.nodes)}\")\n",
    "        print(f\"- Edges: {len(G.edges)}\")\n",
    "        print(f\"- Density: {nx.density(G):.4f}\")\n",
    "        print(f\"- Connected Components: {nx.number_connected_components(G)}\")\n",
    "        \n",
    "        # Sector analysis\n",
    "        sectors = set(nx.get_node_attributes(G, 'sector').values())\n",
    "        print(f\"\\nSectors ({len(sectors)}): {sorted(sectors)}\")\n",
    "        \n",
    "        # Sector-level metrics\n",
    "        print(\"\\nSector Connectivity:\")\n",
    "        sector_stats = []\n",
    "        for sector in sorted(sectors):\n",
    "            nodes = [n for n in G.nodes if G.nodes[n]['sector'] == sector]\n",
    "            sg = G.subgraph(nodes)\n",
    "            sector_stats.append({\n",
    "                'Sector': sector,\n",
    "                'Nodes': len(nodes),\n",
    "                'Edges': sg.number_of_edges(),\n",
    "                'Avg Degree': f\"{2*sg.number_of_edges()/len(nodes):.1f}\" if nodes else \"0\",\n",
    "                'Clustering': f\"{nx.average_clustering(sg):.3f}\" if nodes else \"0\"\n",
    "            })\n",
    "        \n",
    "        print(pd.DataFrame(sector_stats).to_string(index=False))\n",
    "        \n",
    "        # Community detection\n",
    "        print(\"\\nCommunity Detection:\")\n",
    "        communities = nx.algorithms.community.greedy_modularity_communities(G, weight='weight')\n",
    "        print(f\"Found {len(communities)} communities\")\n",
    "        for i, comm in enumerate(sorted(communities, key=len, reverse=True)[:5], 1):\n",
    "            print(f\"Community {i}: {len(comm)} nodes\")\n",
    "            sector_dist = pd.Series([G.nodes[n]['sector'] for n in comm]).value_counts()\n",
    "            print(sector_dist.head(3).to_string())\n",
    "            print(\"---\")\n",
    "        \n",
    "        # Save outputs\n",
    "        nx.write_weighted_edgelist(G, os.path.join(self.output_dir, \"Full_graph.csv\"))\n",
    "        # Save edge list in GraphSAGE-friendly format\n",
    "        pd.DataFrame(\n",
    "            [(u, v, d['weight']) for u, v, d in G.edges(data=True)],\n",
    "            columns=['source', 'target', 'similarity']\n",
    "        ).to_csv(os.path.join(self.output_dir, \"Full_graph.csv\"), index=False)\n",
    "\n",
    "        pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index').to_csv(\n",
    "            os.path.join(self.output_dir, \"node_metadata.csv\"))\n",
    "        \n",
    "        # Visualization\n",
    "        self._visualize_by_sector(G)\n",
    "\n",
    "    def _visualize_by_sector(self, G):\n",
    "        \"\"\"Enhanced sector-colored visualization with communities\"\"\"\n",
    "        plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        # Get sectors and colors\n",
    "        sectors = sorted(set(nx.get_node_attributes(G, 'sector').values()))\n",
    "        colors = plt.cm.tab20(np.linspace(0, 1, len(sectors)))\n",
    "        \n",
    "        # Compute layout\n",
    "        pos = nx.spring_layout(G, k=0.15, seed=42, weight='weight')\n",
    "        \n",
    "        # Draw edges first\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.1, edge_color=\"gray\", width=0.5)\n",
    "        \n",
    "        # Draw nodes by sector\n",
    "        for sector, color in zip(sectors, colors):\n",
    "            nodes = [n for n in G.nodes if G.nodes[n]['sector'] == sector]\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=nodes, node_color=[color], \n",
    "                                 node_size=50, label=sector)\n",
    "        \n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), title=\"Sectors\")\n",
    "        plt.title(\"Stock Network by Sector\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, \"Full_network.png\"), dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    builder = StockGraphBuilder()\n",
    "    G = builder.build_graphs()\n",
    "    \n",
    "    # Verify ISEC\n",
    "    print(\"\\nISEC verification:\", G.nodes[\"ISEC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d73beab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ==== Config ====\n",
    "SHORT_TERM_DIR = \"New_ShortTerm_Stock_Embeddings\"\n",
    "FULL_GRAPH_CSV = \"New_Graphs/Full_Graph.csv\"\n",
    "INTRA_SECTOR_CSV = \"New_Graphs/Intra_Sector_Graph.csv\"\n",
    "OUTPUT_DIR = \"New_GraphSAGE_Embeddings\"\n",
    "EMBEDDING_DIM = 128\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 300\n",
    "LR = 0.005\n",
    "\n",
    "\n",
    "# ==== Step 1: Load and average short-term embeddings ====\n",
    "def load_average_embeddings(short_term_dir):\n",
    "    stock_to_index = {}\n",
    "    sector_map = {}\n",
    "    feature_list = []\n",
    "    expected_dim = None\n",
    "\n",
    "    for idx, filename in enumerate(sorted(os.listdir(short_term_dir))):\n",
    "        if not filename.endswith(\"_embeddings.csv\"):\n",
    "            continue\n",
    "        stock_name = filename.replace(\"_embeddings.csv\", \"\")\n",
    "        df_path = os.path.join(short_term_dir, filename)\n",
    "        df = pd.read_csv(df_path)\n",
    "\n",
    "        if df.empty:\n",
    "            print(f\"⚠️ Skipping {stock_name}: file is empty.\")\n",
    "            continue\n",
    "\n",
    "        emb_cols = [col for col in df.columns if col.startswith('emb_')]\n",
    "        if not emb_cols:\n",
    "            print(f\"⚠️ Skipping {stock_name}: no embedding columns.\")\n",
    "            continue\n",
    "\n",
    "        avg_embedding = df[emb_cols].mean().values.astype(np.float32)\n",
    "\n",
    "        # Debug: show embedding dimension\n",
    "        #print(f\"{stock_name}: embedding dim = {len(avg_embedding)}\")\n",
    "\n",
    "        if len(avg_embedding) == 0:\n",
    "            print(f\"⚠️ Skipping {stock_name}: embedding dim is 0.\")\n",
    "            continue\n",
    "\n",
    "        if expected_dim is None:\n",
    "            expected_dim = len(avg_embedding)\n",
    "        elif len(avg_embedding) != expected_dim:\n",
    "            print(f\"⚠️ Skipping {stock_name}: mismatched embedding shape {len(avg_embedding)} (expected {expected_dim})\")\n",
    "            continue\n",
    "\n",
    "        df['sector'] = df['sector'].fillna('Financial Services')\n",
    "        sector_map[stock_name] = df['sector'].iloc[0]\n",
    "\n",
    "        feature_list.append(avg_embedding)\n",
    "        stock_to_index[stock_name] = len(stock_to_index)\n",
    "\n",
    "    features_tensor = torch.tensor(np.stack(feature_list), dtype=torch.float32)\n",
    "    return features_tensor, stock_to_index, sector_map\n",
    "\n",
    "# ==== Step 2: Load Full Graph edges ====\n",
    "def load_full_graph(full_graph_csv, stock_to_index):\n",
    "    df = pd.read_csv(full_graph_csv)\n",
    "    edges, edge_weights = [], []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        src, tgt = row['source'], row['target']\n",
    "        if src in stock_to_index and tgt in stock_to_index:\n",
    "            edges.append([stock_to_index[src], stock_to_index[tgt]])\n",
    "            edge_weights.append(row['similarity'])\n",
    "\n",
    "    # Fix: Print the number of edges instead of incorrect shapes\n",
    "    print(f\"Number of edges: {len(edges)}\")\n",
    "    \n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    print(f\"edge_index shape: {edge_index.shape}\")\n",
    "    \n",
    "    edge_attr = torch.tensor(edge_weights, dtype=torch.float32)\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "# ==== Step 3: Load Intra-Sector Graph ====\n",
    "def load_intra_sector_graph(intra_sector_csv, stock_to_index, sector_map):\n",
    "    df = pd.read_csv(intra_sector_csv)\n",
    "    edges = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        sector = row['source']\n",
    "        stock = row['target']\n",
    "        for other_stock, other_sector in sector_map.items():\n",
    "            if other_sector == sector and stock != other_stock:\n",
    "                if stock in stock_to_index and other_stock in stock_to_index:\n",
    "                    edges.append([stock_to_index[stock], stock_to_index[other_stock]])\n",
    "\n",
    "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.ones(edge_index.size(1), dtype=torch.float32)\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "\n",
    "# ==== Step 4: Define GraphSAGE Model ====\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ==== Step 5: Training ====\n",
    "def train_graphsage(features, edge_index, edge_attr):\n",
    "    model = GraphSAGE(features.size(1), 256, EMBEDDING_DIM).to(DEVICE)\n",
    "    features = features.to(DEVICE)\n",
    "    edge_index = edge_index.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in tqdm(range(EPOCHS), desc=\"Training GraphSAGE\"):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(features, edge_index)\n",
    "        loss = ((out - features) ** 2).mean()  # identity reconstruction loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(features, edge_index).cpu().numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# ==== Step 6: Save Embeddings ====\n",
    "def save_embeddings(embeddings, stock_to_index, path):\n",
    "    inv_map = {v: k for k, v in stock_to_index.items()}\n",
    "    rows = []\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        row = [inv_map[i]] + list(emb)\n",
    "        rows.append(row)\n",
    "\n",
    "    columns = ['stock'] + [f'emb_{i}' for i in range(embeddings.shape[1])]\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "\n",
    "# ==== Step 6: Run GraphSAGE Model 62 times ====\n",
    "def run_graphsage_pipeline(graph_type='full', run_id=1):\n",
    "    features, stock_to_index, sector_map = load_average_embeddings(SHORT_TERM_DIR)\n",
    "\n",
    "    if graph_type == 'full':\n",
    "        edge_index, edge_attr = load_full_graph(FULL_GRAPH_CSV, stock_to_index)\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"full_graph_embeddings_{run_id}.csv\")\n",
    "    else:\n",
    "        edge_index, edge_attr = load_intra_sector_graph(INTRA_SECTOR_CSV, stock_to_index, sector_map)\n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"intra_sector_graph_embeddings_{run_id}.csv\")\n",
    "\n",
    "    embeddings = train_graphsage(features, edge_index, edge_attr)\n",
    "    save_embeddings(embeddings, stock_to_index, output_file)\n",
    "\n",
    "\n",
    "# ==== Run the training pipeline ====\n",
    "if __name__ == \"__main__\":\n",
    "    for run_id in range(1, 309):  # 1 to 309 inclusive\n",
    "        run_graphsage_pipeline(\"full\", run_id)\n",
    "        run_graphsage_pipeline(\"intra\", run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc8f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from tqdm import tqdm\n",
    "from colorama import Fore, Style\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- Configuration ---\n",
    "class Config:\n",
    "    SEQ_LENGTH = 75  # Reduced from 90 for faster training\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 100  # Reduced from 150\n",
    "    TARGET_COL = 'return_ratio'\n",
    "    FEATURE_COLS = [\n",
    "        'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "        'MA_5','MA_10','MA_20', 'RSI', 'MACD','MACD_signal'  \n",
    "    ]\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    DATA_DIR = \"New_NormalizedProcessedData\"\n",
    "    OUTPUT_DIR = \"New_Predictions\"\n",
    "    EMBEDDINGS_DIR = \"LSTM_Embeddings\"  # New directory for storing embeddings\n",
    "    SHORT_TERM_EMB_DIR = \"New_ShortTerm_Stock_Embeddings\"  # Short-term embeddings directory\n",
    "    EMB_DIMENSION = 128  # Dimension of embeddings to save\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
    "\n",
    "# --- Volatility Dataset ---\n",
    "class VolatilityDataset(Dataset):\n",
    "    def __init__(self, data, seq_length, short_term_emb=None):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "        self.short_term_emb = short_term_emb\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        window = self.data[idx:idx+self.seq_length]\n",
    "        x = window[:, :-1]  # Features\n",
    "        y = window[-1, -1]   # Target\n",
    "        \n",
    "        # Calculate volatility from returns\n",
    "        returns = window[:, -1]\n",
    "        volatility = np.sqrt(np.mean(returns**2)) * 100\n",
    "        \n",
    "        # Add short-term embeddings if available\n",
    "        if self.short_term_emb is not None:\n",
    "            # Make sure we're within bounds\n",
    "            if idx < len(self.short_term_emb):\n",
    "                short_emb = self.short_term_emb[idx]\n",
    "                return (\n",
    "                    torch.FloatTensor(x),\n",
    "                    torch.FloatTensor([y]),\n",
    "                    torch.FloatTensor([volatility]),\n",
    "                    torch.FloatTensor(short_emb)\n",
    "                )\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(x),\n",
    "            torch.FloatTensor([y]),\n",
    "            torch.FloatTensor([volatility])\n",
    "        )\n",
    "\n",
    "# --- Feature Validator ---\n",
    "def validate_features(df, required_cols):\n",
    "    \"\"\"Ensure all required columns exist\"\"\"\n",
    "    missing = [col for col in required_cols if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "    return df[required_cols]\n",
    "\n",
    "# --- Data Processor (Enhanced) ---\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.feature_scaler = RobustScaler()\n",
    "        self.target_scaler = RobustScaler()\n",
    "        \n",
    "    def load_and_preprocess(self, filepath, short_term_emb_dir=None):\n",
    "        df = pd.read_csv(filepath)\n",
    "        stock_name = os.path.basename(filepath).replace(\"_norm.csv\", \"\")\n",
    "        \n",
    "        # Basic feature engineering\n",
    "        df['Log_Volume'] = np.log1p(df['Volume'])\n",
    "        df['Range'] = (df['High'] - df['Low']) / df['Close']\n",
    "        \n",
    "        # Select final features\n",
    "        features = df[Config.FEATURE_COLS + ['Log_Volume', 'Range']]\n",
    "        target = df[Config.TARGET_COL].values.reshape(-1, 1)\n",
    "        \n",
    "        # Load short-term embeddings if available\n",
    "        short_term_emb = None\n",
    "        if short_term_emb_dir:\n",
    "            emb_filepath = os.path.join(short_term_emb_dir, f\"{stock_name}_embeddings.csv\")\n",
    "            if os.path.exists(emb_filepath):\n",
    "                emb_df = pd.read_csv(emb_filepath)\n",
    "                \n",
    "                # Filter embedding columns\n",
    "                emb_cols = [col for col in emb_df.columns if col.startswith('emb_')]\n",
    "                if emb_cols:\n",
    "                    # Convert date to datetime for matching\n",
    "                    emb_df['date'] = pd.to_datetime(emb_df['date'])\n",
    "                    df['Date'] = pd.to_datetime(df['Date'])\n",
    "                    \n",
    "                    # Align embeddings with dataframe by date\n",
    "                    merged_df = pd.merge(df, emb_df[['date'] + emb_cols], \n",
    "                                         left_on='Date', right_on='date', how='left')\n",
    "                    \n",
    "                    # Fill NaN values with zeros (for dates without embeddings)\n",
    "                    for col in emb_cols:\n",
    "                        merged_df[col].fillna(0, inplace=True)\n",
    "                    \n",
    "                    # Extract embeddings as numpy array\n",
    "                    short_term_emb = merged_df[emb_cols].values\n",
    "                    \n",
    "                    print(f\"Loaded {len(emb_cols)} short-term embedding dimensions for {stock_name}\")\n",
    "        \n",
    "        # Scaling\n",
    "        features = self.feature_scaler.fit_transform(features)\n",
    "        target = self.target_scaler.fit_transform(target)\n",
    "        \n",
    "        # Combine scaled features and target\n",
    "        data = np.hstack((features, target))\n",
    "        \n",
    "        # Convert date into the required test set\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        train_end_date = pd.to_datetime(\"2024 -01-10\")\n",
    "        train_mask = df['Date'] <= train_end_date\n",
    "        test_mask = df['Date'] > train_end_date\n",
    "\n",
    "        # Apply split\n",
    "        train_data = data[train_mask.values]\n",
    "        test_data_actual = data[test_mask.values]\n",
    "\n",
    "        # Get padded test data with sequence context\n",
    "        if len(train_data) < Config.SEQ_LENGTH:\n",
    "            raise ValueError(\"Not enough training data to provide sequence context.\")\n",
    "        test_data = np.vstack([train_data[-Config.SEQ_LENGTH:], test_data_actual])\n",
    "\n",
    "        # Prepare test dates: for each test sample we predict, we need to align it with the correct date\n",
    "        # i.e., prediction at time t is made after SEQ_LENGTH timesteps\n",
    "        test_dates = df[test_mask]['Date'].values\n",
    "\n",
    "        print(f\"Target stats - Mean: {target.mean():.6f}, Std: {target.std():.6f}\")\n",
    "        print(f\"Train size: {len(train_data)}, Test size (after padding): {len(test_data)}\")\n",
    "        print(f\"Test prediction days: {len(test_dates)}\")\n",
    "\n",
    "        return train_data, test_data, test_dates, short_term_emb\n",
    "\n",
    "# --- Enhanced Volatility-Sensitive LSTM Model ---\n",
    "class VolatilityLSTM(nn.Module):\n",
    "    def __init__(self, input_size, short_term_emb_size=0):\n",
    "        super().__init__()\n",
    "        self.has_short_term_emb = short_term_emb_size > 0\n",
    "        \n",
    "        # LSTM trunk\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=128,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        \n",
    "        # Volatility attention gate\n",
    "        self.volatility_net = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Short-term embedding integration\n",
    "        if self.has_short_term_emb:\n",
    "            self.short_term_integration = nn.Sequential(\n",
    "                nn.Linear(short_term_emb_size, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, 32)\n",
    "            )\n",
    "            head_input_size = 128 + 32 + 1  # LSTM + short-term + volatility\n",
    "        else:\n",
    "            head_input_size = 129  # LSTM + volatility\n",
    "        \n",
    "        # Prediction head\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(head_input_size, 64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, short_term_emb=None):\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        last_state = lstm_out[:, -1]\n",
    "        \n",
    "        # Get embedding from LSTM (this will be saved)\n",
    "        lstm_embedding = last_state.clone()\n",
    "        \n",
    "        volatility = self.volatility_net(last_state)\n",
    "        \n",
    "        if self.has_short_term_emb and short_term_emb is not None:\n",
    "            # Process short-term embeddings\n",
    "            short_term_features = self.short_term_integration(short_term_emb)\n",
    "            # Combine features\n",
    "            combined = torch.cat([last_state, short_term_features, volatility], dim=1)\n",
    "        else:\n",
    "            combined = torch.cat([last_state, volatility], dim=1)\n",
    "            \n",
    "        prediction = self.head(combined)\n",
    "        \n",
    "        return prediction, volatility, lstm_embedding\n",
    "\n",
    "# --- Model Training (Enhanced) ---\n",
    "def train_model(model, train_loader, has_short_term_emb):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)  # Lower learning rate\n",
    "    criterion = nn.HuberLoss()  # More robust than MSE\n",
    "    \n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            if has_short_term_emb and len(batch) == 4:\n",
    "                x, y, vol, short_emb = [item.to(Config.DEVICE) for item in batch]\n",
    "                optimizer.zero_grad()\n",
    "                pred, pred_vol, _ = model(x, short_emb)\n",
    "            else:\n",
    "                x, y, vol = [item.to(Config.DEVICE) for item in batch]\n",
    "                optimizer.zero_grad()\n",
    "                pred, pred_vol, _ = model(x)\n",
    "            \n",
    "            # Main loss with reduced volatility weight\n",
    "            loss = criterion(pred, y) + 0.1 * F.mse_loss(pred_vol, vol)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)  # Gradient clipping\n",
    "            optimizer.step()\n",
    "            \n",
    "# --- Extract and Save Embeddings ---\n",
    "def extract_embeddings(model, test_loader, test_dates, stock_name, has_short_term_emb):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    volatilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if has_short_term_emb and len(batch) == 4:\n",
    "                x, y, vol, short_emb = [item.to(Config.DEVICE) for item in batch]\n",
    "                pred, pred_vol, emb = model(x, short_emb)\n",
    "            else:\n",
    "                x, y, vol = [item.to(Config.DEVICE) for item in batch]\n",
    "                pred, pred_vol, emb = model(x)\n",
    "            \n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "            predictions.append(pred.cpu().numpy())\n",
    "            actuals.append(y.cpu().numpy())\n",
    "            volatilities.append(pred_vol.cpu().numpy())\n",
    "    \n",
    "    # Concatenate results\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    predictions = np.vstack(predictions)\n",
    "    actuals = np.vstack(actuals)\n",
    "    volatilities = np.vstack(volatilities)\n",
    "    \n",
    "    # Create DataFrame with date and embedding dimensions\n",
    "    emb_df = pd.DataFrame({\n",
    "        'Date': test_dates[:len(embeddings)],\n",
    "        **{f'emb_{i}': embeddings[:, i] for i in range(embeddings.shape[1])},\n",
    "        'Predicted_Return': predictions.flatten(),\n",
    "        'Actual_Return': actuals.flatten(),\n",
    "        'Predicted_Volatility': volatilities.flatten()\n",
    "    })\n",
    "    \n",
    "    # Ensure the embeddings directory exists\n",
    "    os.makedirs(Config.EMBEDDINGS_DIR, exist_ok=True)\n",
    "    \n",
    "    # Save embeddings\n",
    "    emb_filepath = os.path.join(Config.EMBEDDINGS_DIR, f\"{stock_name}_long_term_embeddings.csv\")\n",
    "    emb_df.to_csv(emb_filepath, index=False)\n",
    "    \n",
    "    # Visualize embeddings with PCA\n",
    "    if len(embeddings) > 2:  # Need at least 2 samples for PCA\n",
    "        pca = PCA(n_components=2)\n",
    "        emb_2d = pca.fit_transform(embeddings)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sc = plt.scatter(emb_2d[:, 0], emb_2d[:, 1], \n",
    "                         c=actuals.flatten(), cmap='coolwarm', alpha=0.7)\n",
    "        plt.colorbar(sc, label='Actual Return')\n",
    "        plt.title(f\"Long-Term LSTM Embeddings for {stock_name}\")\n",
    "        plt.xlabel(f\"PCA Component 1 (Var: {pca.explained_variance_ratio_[0]:.2f})\")\n",
    "        plt.ylabel(f\"PCA Component 2 (Var: {pca.explained_variance_ratio_[1]:.2f})\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(Config.EMBEDDINGS_DIR, f\"{stock_name}_long_term_embeddings_viz.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "    print(f\"Saved {len(embeddings)} embeddings for {stock_name}\")\n",
    "    return embeddings, predictions, actuals\n",
    "\n",
    "# --- Main Execution (Enhanced with Embeddings) ---\n",
    "if __name__ == \"__main__\":\n",
    "    processor = DataProcessor()\n",
    "    stock_files = [f for f in os.listdir(Config.DATA_DIR) if f.endswith(\"_norm.csv\")]\n",
    "    \n",
    "    # Dictionary to store results for final visualization\n",
    "    all_embeddings = {}\n",
    "    all_metrics = []\n",
    "    \n",
    "    for stock_file in tqdm(stock_files):\n",
    "        try:\n",
    "            stock_name = stock_file.replace(\"_norm.csv\", \"\")\n",
    "            print(f\"\\n{Fore.CYAN}Processing {stock_name}{Style.RESET_ALL}\")\n",
    "            \n",
    "            # Load data with validation and short-term embeddings\n",
    "            train_data, test_data, test_dates, short_term_emb = processor.load_and_preprocess(\n",
    "                os.path.join(Config.DATA_DIR, stock_file), Config.SHORT_TERM_EMB_DIR)\n",
    "            \n",
    "            # Skip if insufficient data\n",
    "            if len(train_data) < Config.SEQ_LENGTH * 2:\n",
    "                print(f\"{Fore.YELLOW}Skipping {stock_name} (insufficient data){Style.RESET_ALL}\")\n",
    "                continue\n",
    "            \n",
    "            # Determine if we have short-term embeddings\n",
    "            has_short_term_emb = short_term_emb is not None\n",
    "            \n",
    "            # Prepare datasets\n",
    "            train_dataset = VolatilityDataset(train_data, Config.SEQ_LENGTH, \n",
    "                                              short_term_emb[:len(train_data)-Config.SEQ_LENGTH] if has_short_term_emb else None)\n",
    "            test_dataset = VolatilityDataset(test_data, Config.SEQ_LENGTH,\n",
    "                                            short_term_emb[len(train_data)-Config.SEQ_LENGTH:] if has_short_term_emb else None)\n",
    "            \n",
    "            train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False)\n",
    "            \n",
    "            # Initialize model with short-term embedding support if available\n",
    "            short_term_emb_size = short_term_emb.shape[1] if has_short_term_emb else 0\n",
    "            print(f\"Short-term embedding size: {short_term_emb_size}\")\n",
    "            \n",
    "            model = VolatilityLSTM(\n",
    "                input_size=train_data.shape[1] - 1,  # features dimension\n",
    "                short_term_emb_size=short_term_emb_size\n",
    "            ).to(Config.DEVICE)\n",
    "            \n",
    "            # Train\n",
    "            train_model(model, train_loader, has_short_term_emb)\n",
    "            \n",
    "            # Extract embeddings\n",
    "            embeddings, predictions, actuals = extract_embeddings(\n",
    "                model, test_loader, test_dates, stock_name, has_short_term_emb)\n",
    "            \n",
    "            # Inverse scaling for predictions and actuals\n",
    "            predictions = processor.target_scaler.inverse_transform(predictions).flatten()\n",
    "            actuals = processor.target_scaler.inverse_transform(actuals).flatten()\n",
    "            \n",
    "            # Save results\n",
    "            results = pd.DataFrame({\n",
    "                'Date': test_dates[:len(predictions)],\n",
    "                'Actual': actuals,\n",
    "                'Predicted': predictions\n",
    "            })\n",
    "            results.to_csv(f\"{Config.OUTPUT_DIR}/{stock_name}_predictions.csv\", index=False)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = np.mean(np.abs(actuals - predictions))\n",
    "            mse = mean_squared_error(actuals, predictions)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(actuals, predictions)\n",
    "            \n",
    "            # Directional accuracy\n",
    "            directional_accuracy = np.mean(np.sign(actuals) == np.sign(predictions))\n",
    "            \n",
    "            # Store metrics\n",
    "            metrics = {\n",
    "                'stock': stock_name,\n",
    "                'MAE': mae,\n",
    "                'MSE': mse,\n",
    "                'RMSE': rmse,\n",
    "                'R²': r2,\n",
    "                'Directional_Accuracy': directional_accuracy\n",
    "            }\n",
    "            all_metrics.append(metrics)\n",
    "            \n",
    "            print(f\"{Fore.GREEN}Success:{Style.RESET_ALL}\")\n",
    "            print(f\"MAE: {mae:.6f}\")\n",
    "            print(f\"MSE: {mse:.6f}\")\n",
    "            print(f\"RMSE: {rmse:.6f}\")\n",
    "            print(f\"R²: {r2:.3f}\")\n",
    "            print(f\"Directional Accuracy: {directional_accuracy:.3f}\")\n",
    "            print(f\"Prediction Range: [{predictions.min():.6f}, {predictions.max():.6f}]\")\n",
    "            \n",
    "            # Store embeddings for cross-stock analysis\n",
    "            all_embeddings[stock_name] = {\n",
    "                'embeddings': embeddings,\n",
    "                'dates': test_dates[:len(embeddings)]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{Fore.RED}Error processing {stock_file}: {str(e)}{Style.RESET_ALL}\")\n",
    "    \n",
    "    # Create a combined embeddings file for all stocks\n",
    "    try:\n",
    "        combined_data = []\n",
    "        for stock_name, stock_info in all_embeddings.items():\n",
    "            for i, date in enumerate(stock_info['dates']):\n",
    "                row_data = {\n",
    "                    'date': date,\n",
    "                    'stock': stock_name,\n",
    "                    **{f'emb_{j}': stock_info['embeddings'][i, j] for j in range(stock_info['embeddings'].shape[1])}\n",
    "                }\n",
    "                combined_data.append(row_data)\n",
    "        \n",
    "        if combined_data:\n",
    "            combined_df = pd.DataFrame(combined_data)\n",
    "            combined_df.to_csv(f\"{Config.EMBEDDINGS_DIR}/all_stocks_long_term_embeddings.csv\", index=False)\n",
    "            print(f\"Created combined embeddings file with {len(combined_data)} records\")\n",
    "    \n",
    "        # Save metrics summary\n",
    "        if all_metrics:\n",
    "            metrics_df = pd.DataFrame(all_metrics)\n",
    "            metrics_df.to_csv(f\"{Config.OUTPUT_DIR}/all_stocks_metrics.csv\", index=False)\n",
    "            \n",
    "            # Create comparative metrics visualization\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            metrics_df.sort_values('Directional_Accuracy', ascending=False).head(20).plot(\n",
    "                x='stock', y='Directional_Accuracy', kind='bar', color='green', alpha=0.7)\n",
    "            plt.title(\"Top 20 Stocks by Directional Accuracy\")\n",
    "            plt.xlabel(\"Stock\")\n",
    "            plt.ylabel(\"Directional Accuracy\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{Config.OUTPUT_DIR}/top_directional_accuracy.png\")\n",
    "            plt.close()\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"{Fore.RED}Error creating summary files: {str(e)}{Style.RESET_ALL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e8111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import rankdata\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Constants\n",
    "lstm_embeddings_path = 'LSTM_Embeddings'\n",
    "predictions_path = 'New_Predictions'\n",
    "graphsage_dir = 'New_GraphSAGE_Embeddings'\n",
    "K_VALUES = [5, 10, 20]\n",
    "\n",
    "# Date ranges \n",
    "train_start = pd.to_datetime(\"2024-01-11\")\n",
    "train_end = pd.to_datetime(\"2024-10-24\") #80% \n",
    "test1_end = pd.to_datetime(\"2025-1-10\") #20%\n",
    "test2_end = pd.to_datetime(\"2025-04-11\")\n",
    "\n",
    "# Load all 308 graph embeddings\n",
    "full_graphs, intra_graphs = [], []\n",
    "for i in range(1, 309):\n",
    "    full_path = os.path.join(graphsage_dir, f'full_graph_embeddings_{i}.csv')\n",
    "    intra_path = os.path.join(graphsage_dir, f'intra_sector_graph_embeddings_{i}.csv')\n",
    "\n",
    "    full_df = pd.read_csv(full_path, index_col=0).drop(columns=['test_day'], errors='ignore')\n",
    "    intra_df = pd.read_csv(intra_path, index_col=0).drop(columns=['test_day'], errors='ignore')\n",
    "\n",
    "    full_graphs.append(full_df.sort_index())\n",
    "    intra_graphs.append(intra_df.sort_index())\n",
    "\n",
    "# Collect data\n",
    "X_all, y_all, stock_names_all, dates_all = [], [], [], []\n",
    "\n",
    "for stock_file in tqdm(os.listdir(lstm_embeddings_path)):\n",
    "    if not stock_file.endswith('_long_term_embeddings.csv'):\n",
    "        continue\n",
    "\n",
    "    stock_name = stock_file.split('_long_term_embeddings.csv')[0]\n",
    "    lstm_df = pd.read_csv(os.path.join(lstm_embeddings_path, stock_file))\n",
    "    prediction_file = os.path.join(predictions_path, f\"{stock_name}_predictions.csv\")\n",
    "\n",
    "    if not os.path.exists(prediction_file):\n",
    "        continue\n",
    "\n",
    "    lstm_features = lstm_df.drop(columns=['Predicted_Return', 'Actual_Return', 'Predicted_Volatility'], errors='ignore')\n",
    "    lstm_features['Date'] = pd.to_datetime(lstm_df['Date'])\n",
    "\n",
    "    actuals = pd.read_csv(prediction_file)\n",
    "    actuals['Date'] = pd.to_datetime(actuals['Date'])\n",
    "\n",
    "    stock_full_features, stock_returns, stock_dates = [], [], []\n",
    "\n",
    "    for day in range(len(lstm_features)):\n",
    "        date = lstm_features.iloc[day]['Date']\n",
    "        if stock_name not in full_graphs[day].index or stock_name not in intra_graphs[day].index:\n",
    "            continue\n",
    "\n",
    "        lstm_row = lstm_features.iloc[day].drop('Date').values\n",
    "        full_emb = full_graphs[day].loc[stock_name].values\n",
    "        intra_emb = intra_graphs[day].loc[stock_name].values\n",
    "\n",
    "        combined = np.concatenate([lstm_row, full_emb, intra_emb])\n",
    "        stock_full_features.append(combined)\n",
    "        stock_returns.append(actuals.iloc[day]['Actual'])\n",
    "        stock_dates.append(date)\n",
    "\n",
    "    if len(stock_full_features) == 0:\n",
    "        continue\n",
    "\n",
    "    X_all.extend(stock_full_features)\n",
    "    y_all.extend(stock_returns)\n",
    "    stock_names_all.extend([stock_name] * len(stock_full_features))\n",
    "    dates_all.extend(stock_dates)\n",
    "\n",
    "# Convert to arrays\n",
    "X_all = np.array(X_all)\n",
    "y_all = np.array(y_all)\n",
    "dates_all = np.array(dates_all)\n",
    "stock_names_all = np.array(stock_names_all)\n",
    "\n",
    "# Split into train/test1/test2\n",
    "train_mask = (dates_all >= train_start) & (dates_all <= train_end)\n",
    "test1_mask = (dates_all > train_end) & (dates_all <= test1_end)\n",
    "test2_mask = (dates_all > test1_end) & (dates_all <= test2_end)\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_all)\n",
    "\n",
    "# Train\n",
    "model = XGBRegressor(objective='reg:squarederror', eval_metric='rmse', n_estimators=1000, max_depth=8, learning_rate=0.1)\n",
    "model.fit(X_scaled[train_mask], y_all[train_mask])\n",
    "\n",
    "# Predict\n",
    "y_pred_all = model.predict(X_scaled)\n",
    "\n",
    "# Ranking Metrics (Revised based on baseline definitions)\n",
    "def mrr_at_k(actual, pred, k=5):\n",
    "    df = pd.DataFrame({'Actual': actual, 'Predicted': pred})\n",
    "    df = df.sort_values(by='Predicted', ascending=False).reset_index(drop=True)\n",
    "    df['Pred_Rank_Index'] = df.index + 1  # Ranking from prediction\n",
    "    df = df.sort_values(by='Actual', ascending=False).reset_index(drop=True)\n",
    "    top_k_actual_pred_ranks = df['Pred_Rank_Index'][:k]\n",
    "    return np.mean(1 / top_k_actual_pred_ranks)\n",
    "\n",
    "def precision_at_k(actual, pred, k=5):\n",
    "    df = pd.DataFrame({'Actual': actual, 'Predicted': pred})\n",
    "    top_k_pred_idx = df.sort_values(by='Predicted', ascending=False).index[:k]\n",
    "    top_k_actual_idx = df.sort_values(by='Actual', ascending=False).index[:k]\n",
    "    intersection = len(set(top_k_pred_idx) & set(top_k_actual_idx))\n",
    "    return intersection / k\n",
    "\n",
    "def irr_at_k(actual, pred, k=5):\n",
    "    df = pd.DataFrame({'Actual': actual, 'Predicted': pred})\n",
    "    top_k_actual_sum = df.sort_values(by='Actual', ascending=False)['Actual'][:k].sum()\n",
    "    top_k_pred_sum = df.sort_values(by='Predicted', ascending=False)['Actual'][:k].sum()\n",
    "    return top_k_actual_sum - top_k_pred_sum\n",
    "\n",
    "\n",
    "def evaluate_subset(mask, label):\n",
    "    subset_dates = dates_all[mask]\n",
    "    subset_actual = y_all[mask]\n",
    "    subset_pred = y_pred_all[mask]\n",
    "    subset_stocks = stock_names_all[mask]\n",
    "\n",
    "    results = []\n",
    "    daily_rankings = []\n",
    "\n",
    "    for d in sorted(set(subset_dates)):\n",
    "        idx = (subset_dates == d)\n",
    "        actual_day = subset_actual[idx]\n",
    "        pred_day = subset_pred[idx]\n",
    "        stock_names_day = subset_stocks[idx]\n",
    "\n",
    "        ranked_idx = np.argsort(pred_day)[::-1]\n",
    "        ranked_stocks = np.array(stock_names_day)[ranked_idx]\n",
    "        ranked_actuals = actual_day[ranked_idx]\n",
    "        ranked_preds = pred_day[ranked_idx]\n",
    "\n",
    "        for rank_num, stock in enumerate(ranked_stocks[:20], 1):\n",
    "            daily_rankings.append({\n",
    "                'Date': d,\n",
    "                'Rank': rank_num,\n",
    "                'Stock': stock,\n",
    "                'Actual': ranked_actuals[rank_num - 1],\n",
    "                'Predicted': ranked_preds[rank_num - 1]\n",
    "            })\n",
    "\n",
    "        result = {\n",
    "            'Date': d,\n",
    "            'MSE': mean_squared_error(actual_day, pred_day),\n",
    "            'MAE': mean_absolute_error(actual_day, pred_day),\n",
    "        }\n",
    "\n",
    "        for k in K_VALUES:\n",
    "            result[f'MRR@{k}'] = mrr_at_k(actual_day, pred_day, k)\n",
    "            result[f'Precision@{k}'] = precision_at_k(actual_day, pred_day, k)\n",
    "            result[f'IRR@{k}'] = irr_at_k(actual_day, pred_day, k)\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    # Build daily metrics dataframe\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    metrics_df.to_csv(f\"{label}_metrics.csv\", index=False)\n",
    "\n",
    "    # === Print Mean Metrics ===\n",
    "    mean_row = metrics_df.mean(numeric_only=True).round(6)\n",
    "    print(f\"\\n📈 [Mean Metrics for {label.upper()} Set]\")\n",
    "    print(mean_row.to_frame().T.to_string(index=False))\n",
    "\n",
    "    # === Find Best Daily Metrics and Dates ===\n",
    "    best_metrics_records = []\n",
    "\n",
    "    for k in K_VALUES:\n",
    "        best_mrr_value = metrics_df[f'MRR@{k}'].max()\n",
    "        best_mrr_date = metrics_df.loc[metrics_df[f'MRR@{k}'].idxmax(), 'Date']\n",
    "\n",
    "        best_precision_value = metrics_df[f'Precision@{k}'].max()\n",
    "        best_precision_date = metrics_df.loc[metrics_df[f'Precision@{k}'].idxmax(), 'Date']\n",
    "\n",
    "        best_irr_value = metrics_df[f'IRR@{k}'].min()\n",
    "        best_irr_date = metrics_df.loc[metrics_df[f'IRR@{k}'].idxmin(), 'Date']\n",
    "\n",
    "        best_metrics_records.append({\n",
    "            'Metric': f'MRR@{k}',\n",
    "            'Best_Value': round(best_mrr_value, 6),\n",
    "            'Best_Date': pd.to_datetime(best_mrr_date).date()\n",
    "        })\n",
    "        best_metrics_records.append({\n",
    "            'Metric': f'Precision@{k}',\n",
    "            'Best_Value': round(best_precision_value, 6),\n",
    "            'Best_Date': pd.to_datetime(best_precision_date).date()\n",
    "        })\n",
    "        best_metrics_records.append({\n",
    "            'Metric': f'IRR@{k}',\n",
    "            'Best_Value': round(best_irr_value, 6),\n",
    "            'Best_Date': pd.to_datetime(best_irr_date).date()\n",
    "        })\n",
    "\n",
    "    best_metrics_df = pd.DataFrame(best_metrics_records)\n",
    "\n",
    "    # === Print Best Daily Metrics ===\n",
    "    print(f\"\\n🏆 [Best Daily Metrics for {label.upper()} Set]\")\n",
    "    print(best_metrics_df.to_string(index=False))\n",
    "\n",
    "    # Save best metrics to file\n",
    "    best_metrics_df.to_csv(f\"{label}_best_daily_metrics.csv\", index=False)\n",
    "\n",
    "    # Save daily rankings (Top 20 stocks)\n",
    "    daily_rankings_df = pd.DataFrame(daily_rankings)\n",
    "    daily_rankings_df.to_csv(f\"{label}_daily_rankings.csv\", index=False)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'Date': subset_dates,\n",
    "        'Stock': subset_stocks,\n",
    "        'Actual': subset_actual,\n",
    "        'Predicted': subset_pred\n",
    "    })\n",
    "    \n",
    "# Function to save daily top-k rankings\n",
    "def save_daily_topk_rankings(df_preds, label):\n",
    "    K_VALUES = [5, 10, 20]\n",
    "    dates = pd.to_datetime(df_preds['Date'].unique())\n",
    "\n",
    "    records = []\n",
    "\n",
    "    for d in tqdm(sorted(dates), desc=f\"Saving Top-K Rankings ({label})\"):\n",
    "        daily_df = df_preds[df_preds['Date'] == d]\n",
    "\n",
    "        # Sort by Predicted\n",
    "        daily_pred_sorted = daily_df.sort_values('Predicted', ascending=False)\n",
    "\n",
    "        # Sort by Actual\n",
    "        daily_actual_sorted = daily_df.sort_values('Actual', ascending=False)\n",
    "\n",
    "        for k in K_VALUES:\n",
    "            topk_predicted = daily_pred_sorted.head(k)\n",
    "            topk_actual = daily_actual_sorted.head(k)\n",
    "\n",
    "            for rank_idx, row in enumerate(topk_predicted.itertuples(), start=1):\n",
    "                records.append({\n",
    "                    'Date': d,\n",
    "                    'Type': f'Predicted_Top{k}',\n",
    "                    'Rank': rank_idx,\n",
    "                    'Stock': row.Stock,\n",
    "                    'Predicted': row.Predicted,\n",
    "                    'Actual': row.Actual\n",
    "                })\n",
    "\n",
    "            for rank_idx, row in enumerate(topk_actual.itertuples(), start=1):\n",
    "                records.append({\n",
    "                    'Date': d,\n",
    "                    'Type': f'Actual_Top{k}',\n",
    "                    'Rank': rank_idx,\n",
    "                    'Stock': row.Stock,\n",
    "                    'Predicted': row.Predicted,\n",
    "                    'Actual': row.Actual\n",
    "                })\n",
    "\n",
    "    rankings_df = pd.DataFrame(records)\n",
    "    rankings_df.to_csv(f\"{label}_daily_topk_rankings.csv\", index=False)\n",
    "    print(f\"✅ Saved {label}_daily_topk_rankings.csv successfully.\")\n",
    "\n",
    "\n",
    "# Evaluate both test sets\n",
    "preds_test1 = evaluate_subset(test1_mask, 'Old_Test')\n",
    "preds_test2 = evaluate_subset(test2_mask, 'New_Test')\n",
    "\n",
    "# Save all predictions\n",
    "pd.DataFrame({\n",
    "    'Date': dates_all,\n",
    "    'Stock': stock_names_all,\n",
    "    'Actual': y_all,\n",
    "    'Predicted': y_pred_all\n",
    "}).to_csv(\"final_predictions.csv\", index=False)\n",
    "\n",
    "print(\"✅ All prediction and metric files saved.\")\n",
    "\n",
    "# Save the daily Top-K rankings\n",
    "save_daily_topk_rankings(preds_test1, 'Old_Test')\n",
    "save_daily_topk_rankings(preds_test2, 'New_test')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
